% Langevin Particle Autoencoders documentation master file, created by
% sphinx-quickstart on Tue Apr 18 08:42:26 2023.
% You can adapt this file completely to your liking, but it should at least
% contain the root `toctree` directive.

%```{include} ../../README.md
%```

**L**angevin **P**article **A**uto**E**ncoders: A simple and fast tensorflow 
implementation of the autoencoder models in 
'[Particle algorithms for maximum likelihood training of latent 
variable models](https://proceedings.mlr.press/v206/kuntz23a.html)'.

```{toctree}
:caption: 'Contents:'
:maxdepth: 2

```
%API_reference

## Model

These autoencoders assume that each datapoint {math}`y^m` in a dataset 
$y^{1:M}=(y^m)_{m=1}^M$ is independently generated by:

1.  sampling latent variables $x^m$ from a zero mean, unit variance Gaussian 
distribution on $\mathbb{R}^{d_x}$,
2.  mapping them through a 'decoder' or 'generator' neural network $f_\theta$ 
to the data space $\mathbb{R}^{d_y}$,
3.  and adding Gaussian noise with variance $\sigma^2$.

In full, the model reads

$$p_\theta (x^{1:M},y^{1:M}) = \prod_{m=1}^M p_\theta(x^{m}, y^{m}),$$

where $p_\theta(x^m,y^m)= p_\theta(y^m|x^m)p(x^m)$ with

$$p_\theta(y^m|x^m) := \mathcal{N}(y^m|f_\theta(x^m), \sigma^2 I),\quad p(x^m):=\mathcal{N}(x^m|0,I),\quad\forall m\in[M].$$

Hence,

$$\ell(\theta,x^{1:M})=\log(p_\theta(x^{1:M},y^{1:M}))=\sum_{m=1}^M\ell(\theta,x^m),$$

where $\ell(\theta,x^m):= \log(p_\theta(x^m,y^m))$ for all $m$ in $[M]$.

## Training 

We fit the decoder's weights and biases, collected in $\theta$, by 
(approximately) maximizing the marginal likelihood (the probability of 
observing the data we observed according to the model):

$$\theta\mapsto p_\theta(y):=\int p_\theta(x,y)dx.$$

To do so, we use particle  gradient descent (PGD, Algorithm 1 in 
[here](https://proceedings.mlr.press/v206/kuntz23a.html)), subsampled and with 
adaptive step-sizes:

{math}`\begin{align*}\Theta_{k+1} &= \Theta_{k} + H_k\sum_{n=1}^N  \sum_{m\in\mathcal{B}_k}\nabla_\theta\ell(\Theta_k,X_k^{n,m}),\quad\forall k\in[K],\\
X_{k+1}^{n,m}&=X_k^{n,m}+h\nabla_x \ell(\Theta_k,X_k^{n,m})+\sqrt{2h}W_k^{n,m},\quad\forall m\in\mathcal{B}_k,\quad n\in[N],\quad  k\in[K].
\end{align*}`

where {math}`(\mathcal{B}_k)_{k\in[K]}` denotes random batches of indices with 
batch size $M_\mathcal{B}:=|\mathcal{B}|$, and we pick $(H_k)_{k\in[K]}$ as 
in [RMSProp](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/RMSprop).
See the [paper](https://proceedings.mlr.press/v206/kuntz23a.html)'s Appendix 
E.4 for more details on these sorts of PGD variants.

## Installation

To check out the repository out on Colab, follow the steps taken in 
[this tutorial](notebooks/MNIST_tutorial). To use the code locally, download 
the repository and create an environment with the required packages installed 
(the code was tested on python 3.10):

```
git clone https://github.com/juankuntz/LPAE.git
cd LPAE
python -m venv lpae_env
source lpae_env
python -m pip install -r requirements.txt
```

## Usage

For a quick overview, check out [this tutorial](notebooks/MNIST_tutorial). 
For more info, see the {doc}`API_reference`.

## Citation
If you find the code useful for your research, please consider citing our 
paper:

```bib
@InProceedings{Kuntz2023,
  title = 	 {Particle algorithms for maximum likelihood training of latent variable models},
  author =       {Kuntz, Juan and Lim, Jen Ning and Johansen, Adam M.},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {5134--5180},
  year = 	 {2023},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  url = 	 {https://proceedings.mlr.press/v206/kuntz23a.html},
}
```
%# Indices and tables

%- {ref}`genindex`
%- {ref}`modindex`
%- {ref}`search`
